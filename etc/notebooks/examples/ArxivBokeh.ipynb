{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arxiv Concept Explorer\n",
    "\n",
    "**[Arxiv](http://arxiv.org/)** is a site where researchers can post research papers at any time. The site can contain some of the **most recent, state of the art research**, since papers may be posted prior to conference or journal submission deadlines. Arxiv also allows open access for all of its papers, spanning many disciplines.\n",
    "\n",
    "In this notebook, we develop an application for **discovering high-level concepts and keywords** that appear in Arxiv papers, and **retrieving papers** relevant to the concepts. The application serves as a way of discovering emerging ideas, surveying the prominent themes in a field, or efficiently navigating through the wealth of content on the site.\n",
    "\n",
    "The notebook uses **Declarative Widgets** and the **Bokeh visualization library**, demonstrating how these two tools can be used together to build powerful interactive notebook applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------\n",
    "### Setup\n",
    "\n",
    "Initialize the declarative widgets extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urth import widgets\n",
    "\n",
    "widgets.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application requires an `AlchemyAPI` API key ([available here](http://www.alchemyapi.com/api/register.html)).\n",
    "\n",
    "Run the next 3 cells and use the widget below to enter your key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<!-- Import Dependencies -->\n",
    "<link rel=\"import\" href=\"urth_components/urth-viz-table/urth-viz-table.html\" is=\"urth-core-import\">\n",
    "<link rel=\"import\" href=\"urth_components/paper-input/paper-input.html\" is='urth-core-import' package='PolymerElements/paper-input'>\n",
    "<link rel=\"import\" href=\"urth_components/paper-button/paper-button.html\" is='urth-core-import' package='PolymerElements/paper-button'>\n",
    "<link rel=\"import\" href=\"urth_components/paper-icon-button/paper-icon-button.html\" is='urth-core-import' package='PolymerElements/paper-icon-button'>\n",
    "<link rel=\"import\" href=\"urth_components/paper-progress/paper-progress.html\" is='urth-core-import' package='PolymerElements/paper-progress'>\n",
    "\n",
    "<!-- Define data channels -->\n",
    "<urth-core-channel name='plot' id='plotChannel'></urth-core-channel>\n",
    "<urth-core-channel name='status' id='statusChannel'></urth-core-channel>\n",
    "<urth-core-channel name='table' id='tableChannel'></urth-core-channel>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_key(key):\n",
    "    global ALCHEMY_API_KEY\n",
    "    ALCHEMY_API_KEY = key\n",
    "    print(\"AlchemyAPI key set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "Please input your AlchemyAPI key and click \"Set\":\n",
    "<template is=\"dom-bind\">\n",
    "    <urth-core-function id=\"setkey\" ref='set_key' arg-key=\"{{key}}\"></urth-core-function>\n",
    "    <paper-input label=\"AlchemyAPI Key\" value=\"{{key}}\"></paper-input>\n",
    "    <paper-button raised onclick=\"setkey.invoke()\">\n",
    "      Set\n",
    "    </paper-button>\n",
    "</template>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll install and import the necessary libraries. We assume that `conda` can be used to install `Bokeh`, or `Bokeh` is already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!conda install -y bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import feedparser\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import operator\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.charts import Histogram\n",
    "from bokeh.models import CustomJS\n",
    "from bokeh.models.renderers import GlyphRenderer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from urth.widgets.widget_channels import channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "We'll be downloading the `NUM_PAPERS` most recent Arxiv papers using the [Arxiv API](http://arxiv.org/help/api/user-manual):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'http://export.arxiv.org/api/query?'\n",
    "\n",
    "# Number of papers per Arxiv API request.\n",
    "PAGE_SIZE = 100\n",
    "\n",
    "# Maximum number of papers to download.\n",
    "NUM_PAPERS = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is used to cache the papers downloaded from Arxiv, making repeat searches quicker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entry_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "The next few cells contain numerous functions for retrieving, searching, plotting, and displaying the Arxiv data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving Data Functions\n",
    "\n",
    "This cell contains functions responsible for interacting with the Arxiv API, AlchemyAPI, and post-processing of the text and results.\n",
    "\n",
    "AlchemyAPI's `TextGetRankedConcepts` and `TextGetRankedKeywords` endpoints are used for finding concepts and keywords in the downloaded paper data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_entries(query, max_results=10000, use_cache=True):\n",
    "    \"\"\" \n",
    "    Downloads entries for the given query term from Arxiv.\n",
    "    \n",
    "    Entries are retrieved in PAGE_SIZE chunks. A delay of\n",
    "    3 seconds is introduced between each API call per Arxiv\n",
    "    API guidelines.\n",
    "        \n",
    "    :param query: String to query for\n",
    "    :param max_results: Integer\n",
    "    :param use_cache: Cache downloaded entries for queries when True\n",
    "    :return: List of dictionaries, where each dictionary is an Arxiv entry.\n",
    "    \"\"\"\n",
    "    query = query.replace(' ', '+')\n",
    "    if use_cache and query in entry_cache:\n",
    "        channel('status').set('status', \"Using cached data.\")\n",
    "        channel('status').set('progress', 100)\n",
    "        return entry_cache[query]\n",
    "    \n",
    "    entries = []\n",
    "    \n",
    "    iters = int(max(max_results / PAGE_SIZE, 1))\n",
    "    for i in range(iters):\n",
    "        start = i*PAGE_SIZE\n",
    "        q = build_query(query, start, PAGE_SIZE)\n",
    "        channel('status').set('status', \"Retrieving papers from Arxiv. Downloading results {}-{} out of {}...\".format(\n",
    "                start+1, min(start+PAGE_SIZE, max_results), max_results)\n",
    "             )\n",
    "        channel('status').set('progress', start / max_results * 100)\n",
    "        response = urllib.request.urlopen(base_url+q).read()\n",
    "        feed = feedparser.parse(response)\n",
    "        if len(feed.entries) == 0:\n",
    "            break\n",
    "        entries.extend(feed.entries)\n",
    "        \n",
    "        # to be nice to the Arxiv servers\n",
    "        time.sleep(3)\n",
    "    channel('status').set('status', \"Downloaded {} entries.\".format(len(entries)))\n",
    "    channel('status').set('progress', 100)\n",
    "    entry_cache[query] = entries\n",
    "    return entries\n",
    "\n",
    "def build_query(query, start, max_results): \n",
    "    \"\"\" Constructs the Arxiv API query. \"\"\"\n",
    "    return 'search_query={}&start={}&max_results={}&sortBy=submittedDate&sortOrder=descending'.format(query,\n",
    "                                                         start,\n",
    "                                                         max_results)\n",
    "\n",
    "def _clean_text(txt):\n",
    "    \"\"\" \n",
    "    :param txt: A String of text.\n",
    "    :return: A String of cleaned text, words space-separated.\n",
    "    \"\"\"\n",
    "    alpha_only = re.sub(\"[^a-zA-Z\\s]+\", \"\", txt.replace('\\n', ' ')).lower()\n",
    "    return alpha_only\n",
    "\n",
    "def get_text(entries):\n",
    "    \"\"\" \n",
    "    Retrieve the test to be used as input to Alchemy API.\n",
    "    \n",
    "    :param entries: List of Arxiv entries [{entry_1}, ..., {entry_n}]\n",
    "    :return: A String containing the text from all entries.\n",
    "    \"\"\"\n",
    "    entry_text = []\n",
    "    for e in entries:\n",
    "        entry_text.append(_clean_text(e.title))\n",
    "        # Exclude due to AlchemyAPI truncation\n",
    "        # entry_text.append(_clean_text(e.summary))\n",
    "    all_text = ' '.join(entry_text)\n",
    "    return all_text\n",
    "\n",
    "def _alchemy_api_call(txt, url):\n",
    "    \"\"\" \n",
    "    Make an Alchemy API POST request using the given text.\n",
    "    \n",
    "    :param txt: Text to send\n",
    "    :param url: Url to POST to\n",
    "    :return: List of dictionaries, where each dictionary is a concept.\n",
    "    \"\"\"\n",
    "    base = url\n",
    "    params = urllib.parse.urlencode(dict(\n",
    "            apikey=ALCHEMY_API_KEY, text=txt, outputMode='json'))\n",
    "    req = urllib.request.Request(base, bytes(params, 'ascii'))\n",
    "    response = urllib.request.urlopen(req).read()\n",
    "    results = json.loads(response.decode('utf-8'))\n",
    "    return results\n",
    "\n",
    "def concepts_api_call(txt):\n",
    "    \"\"\" \n",
    "    Retrieve concepts for the given string of text using Alchemy API.\n",
    "    \n",
    "    :param txt: Text to send\n",
    "    :return: List of dictionaries, where each dictionary represents a concept.\n",
    "    \"\"\"\n",
    "    base = \"http://gateway-a.watsonplatform.net/calls/text/TextGetRankedConcepts\"\n",
    "    return _alchemy_api_call(txt, base)\n",
    "\n",
    "def keywords_api_call(txt):\n",
    "    \"\"\" \n",
    "    Retrieve keywords for the given string of text using Alchemy API.\n",
    "    \n",
    "    :param txt: Text to send\n",
    "    :return: List of dictionaries, where each dictionary represents a keyword.\n",
    "    \"\"\"\n",
    "    base = \"http://gateway-a.watsonplatform.net/calls/text/TextGetRankedKeywords\"\n",
    "    return _alchemy_api_call(txt, base)\n",
    "\n",
    "def get_concepts(entries):\n",
    "    \"\"\" \n",
    "    Retrieve concepts for the given Arxiv entries.\n",
    "    \n",
    "    :param entries: List of Arxiv entries represented as dictionaries.\n",
    "    :return: List of dictionaries, where each dictionary represents a concept.\n",
    "    \"\"\"\n",
    "    txt = get_text(entries)\n",
    "    return concepts_api_call(txt)\n",
    "\n",
    "def get_keywords(entries):\n",
    "    \"\"\" \n",
    "    Retrieve keywords for the given Arxiv entries.\n",
    "    \n",
    "    :param entries: List of Arxiv entries represented as dictionaries.\n",
    "    :return: List of dictionaries, where each dictionary represents a keyword.\n",
    "    \"\"\"\n",
    "    txt = get_text(entries)\n",
    "    return keywords_api_call(txt)\n",
    "\n",
    "def _plot_data(results_dict, kind):\n",
    "    xs = [c['text'] for c in results_dict[kind]]\n",
    "    ys = [float(c['relevance']) for c in results_dict[kind]]\n",
    "    return (xs, ys)   \n",
    "\n",
    "def keywords_plot_data(keywords_dict):\n",
    "    \"\"\" \n",
    "    Format data from a keywords dictionary for use in a plot.\n",
    "    \n",
    "    :param concepts_dict: Dictionary representing a keyword.\n",
    "    :return: ([keyword labels], [relevance scores])\n",
    "    \"\"\"\n",
    "    return _plot_data(keywords_dict, 'keywords')\n",
    "\n",
    "def concepts_plot_data(concepts_dict):\n",
    "    \"\"\" \n",
    "    Format data from a concept dictionary for use in a plot.\n",
    "    \n",
    "    :param concepts_dict: Dictionary representing a concept.\n",
    "    :return: ([concept labels], [relevance scores])\n",
    "    \"\"\"\n",
    "    return _plot_data(concepts_dict, 'concepts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching Data Functions\n",
    "\n",
    "This cell contains functions for finding papers that are relevant to a concept or keyword. We use [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) scoring to retrieve the relevant papers.\n",
    "\n",
    "Note the `top_docs_for_query` function, which performs the search for relevant documents. This function will be used in the `on_plot_click` handler below, and returns results in a form that works with the `urth-viz-table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_docs_for_query(query, entries, tf_idf_index, n=100):\n",
    "    \"\"\"\n",
    "    Finds the most relevant documents for a given query.\n",
    "    \n",
    "    A document is represented as a list of several 'columns' that\n",
    "    can be consumed by a table. Thus, the function also returns the\n",
    "    column schema.\n",
    "    \n",
    "    Only results with positive relevance are returned.\n",
    "    \n",
    "    :param query: String concept or keyword to find relevant docs for.\n",
    "    :param entries: List of dicts, where each dict is an Arxiv entry\n",
    "    :param tf_idf_index: Dictionary, Index used to compute document relevance\n",
    "    :param n: Maximum number of results to return.\n",
    "    \"\"\"\n",
    "    results = find_relevances_for_query(query, entries, tf_idf_index)\n",
    "    sorted_results = sorted(results.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    relevances = compute_relevances(sorted_results)\n",
    "    id_relevance_pairs = [(pair[0], rel) for pair, rel in zip(sorted_results, relevances)]\n",
    "    data = [DocResult(eid, rel).table_data() for eid, rel in id_relevance_pairs if rel > 0.0][:n]\n",
    "    return data, DocResult.columns\n",
    "\n",
    "def _datestring(struct_date):\n",
    "    return \"{}/{}/{}\".format(\n",
    "        struct_date.tm_mon, struct_date.tm_mday, struct_date.tm_year)\n",
    "\n",
    "def get_entry_by_id(eid, entries):\n",
    "    for e in entries:\n",
    "        if e.id == eid:\n",
    "            return e\n",
    "\n",
    "def find_relevances_for_query(query, entries, tf_idf_index):\n",
    "    \"\"\"\n",
    "    Computes the relevance of each entry to the given query.\n",
    "    \n",
    "    A document's relevance is determined by the total tf-idf \n",
    "    score of the query words with respect to the document.\n",
    "    \n",
    "    :param query: String\n",
    "    :param entries: List of dicts\n",
    "    :tf_idf_index: Dictionary, Index used to compute relevance\n",
    "    :return: Dictionary mapping entry_id to relevance score\n",
    "    \"\"\"\n",
    "    entry_rels = defaultdict(int)\n",
    "    for i, entry in enumerate(entries):\n",
    "        words = []\n",
    "        tfs = []\n",
    "        for word in query.split(' '):\n",
    "            tf_idf_score = tf_idf_index[(i, word.lower())]\n",
    "            entry_rels[entry.id] += tf_idf_score\n",
    "    return entry_rels\n",
    "\n",
    "def compute_relevances(id_rel_pairs):\n",
    "    \"\"\"\n",
    "    Finds relevance score given [(entry_id, relevance)]\n",
    "    \"\"\"\n",
    "    rels = [rel for _, rel in id_rel_pairs]\n",
    "    max_r = max(rels)\n",
    "    return [rel / max_r for rel in rels]\n",
    "\n",
    "\n",
    "def _create_doc(entry, exclude=[]):\n",
    "    \"\"\"\n",
    "    Creates a bag-of-words representation of an\n",
    "    entry's text.\n",
    "    \n",
    "    :param entry: An Arxiv entry dictionary\n",
    "    :param exclude: List of words to exclude in the\n",
    "                    representation\n",
    "    :return: Dictionary representing the entry's text\n",
    "             of the form words[word] = frequency\n",
    "    \"\"\"\n",
    "    words = defaultdict(float)\n",
    "    for section in [entry.title, entry.summary]:\n",
    "        text = _clean_text(section).split(' ')\n",
    "        for word in text:\n",
    "            if word not in exclude and word != '':\n",
    "                words[word] += 1.0\n",
    "    return words\n",
    "\n",
    "def _get_vocab(docs):\n",
    "    \"\"\"\n",
    "    :param docs: List of document dictionaries\n",
    "    :return: Set of unique word tokens in docs\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    for doc in docs:\n",
    "        for word in doc.keys():\n",
    "            vocab.add(word)\n",
    "    return vocab\n",
    "\n",
    "def build_tf_idf_index(entries):\n",
    "    \"\"\"\n",
    "    Builds an index with tf-idf scores for all\n",
    "    (entry_number, word) pairs. Words are extracted from each entry\n",
    "    using the _create_doc function.\n",
    "    \n",
    "    Entry numbers are equivalent to the entry's index in `entries`.\n",
    "    \n",
    "    :param entries: list of dicts, where each dict is an Arxiv entry\n",
    "    :return: Dictionary index with tf-idf scores, of the form:\n",
    "             index[(entry_number, word)] = float tf_idf score\n",
    "    \"\"\"\n",
    "    channel('status').set('status', \"Building tf-idf index...\")\n",
    "    channel('status').set('progress', 0)\n",
    "    index = defaultdict(float)\n",
    "    docs = [_create_doc(e) for e in entries]\n",
    "    vocab = _get_vocab(docs)\n",
    "    num_docs = len(docs)\n",
    "    for i, doc in enumerate(docs):\n",
    "        for word in vocab:\n",
    "            if word in doc:\n",
    "                index[(i, word)] = tf_idf(word, i, docs)\n",
    "        if i % 100 == 0:\n",
    "            channel('status').set('progress', (i + 1) / num_docs * 100)\n",
    "    channel('status').set('progress', 100)\n",
    "    channel('status').set('status', \"tf-idf index built.\")\n",
    "    return index\n",
    "\n",
    "def tf_idf(term, entry_index, docs):\n",
    "    \"\"\"\n",
    "    Computes the tf-idf score for the term wrt the\n",
    "    document list, using the index.\n",
    "    \"\"\"\n",
    "    return tf(term, docs[entry_index])*idf(term, docs)\n",
    "\n",
    "def tf(term, document):\n",
    "    \"\"\"\n",
    "    Number of times the term appears in the document.\n",
    "    \"\"\"\n",
    "    return document[term]\n",
    "\n",
    "def idf(term, documents):\n",
    "    \"\"\"\n",
    "    Inverse document frequency of the term for\n",
    "    the list of documents.\n",
    "    \"\"\"\n",
    "    n = len(documents)\n",
    "    return np.log(n * 1.0 / df(term, documents))\n",
    "\n",
    "def df(term, documents):\n",
    "    \"\"\"\n",
    "    Number of documents the term appears in.\n",
    "    \"\"\"\n",
    "    n = 0\n",
    "    for doc in documents:\n",
    "        if term in doc:\n",
    "            n += 1\n",
    "    return n\n",
    "\n",
    "class DocResult:\n",
    "    \"\"\"\n",
    "    Represents a retrieved document.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Column names for the table_data representation\n",
    "    columns = ['Title', 'Author', 'Date', 'Link', 'Relevance']\n",
    "    \n",
    "    def __init__(self, entry_id, relevance):\n",
    "        self.entry = get_entry_by_id(entry_id, entries)\n",
    "        self.relevance = relevance\n",
    "        \n",
    "    def table_data(self):\n",
    "        \"\"\"\n",
    "        Represents this DocResult in a form consumable\n",
    "        by a table.\n",
    "        \"\"\"\n",
    "        row = []\n",
    "        for column in self.columns:\n",
    "            if column == 'Date':\n",
    "                row.append(_datestring(self.entry['published_parsed']))\n",
    "            elif column == 'Relevance':\n",
    "                row.append(self.relevance)\n",
    "            else:\n",
    "                row.append(self.entry[column.lower()])\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Functions\n",
    "\n",
    "Now we define the two Bokeh plots used in the notebook, and functions for updating plots that have already been rendered.\n",
    "\n",
    "Note the use of `push_notebook` to dynamically update the Bokeh plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _compute_bars(ys):\n",
    "    bar_width = 0.2\n",
    "    tops = ys\n",
    "    bottoms = [0 for y in ys]\n",
    "    lefts = [x-bar_width + 1 for x in np.arange(0, len(ys))]\n",
    "    rights = [x+bar_width + 1 for x in np.arange(0, len(ys))]\n",
    "    return (tops, bottoms, lefts, rights)\n",
    "\n",
    "def _compute_text_positions(ys):\n",
    "    x = [x - 0.2 for x in np.arange(1, len(ys)+1)]\n",
    "    y = [y + 0.05 for y in ys]\n",
    "    return (x, y)\n",
    "\n",
    "def update_relevances_plot(xs, ys, p):\n",
    "    \"\"\"\n",
    "    Update the given relevance plot with new data.\n",
    "    \n",
    "    xs: List of bar labels\n",
    "    ys: List of relevance scores\n",
    "    p: A rendered relevance plot\n",
    "    \"\"\"\n",
    "    rend = p.select(dict(type=GlyphRenderer))[0]\n",
    "    tops, bottoms, lefts, rights = _compute_bars(ys)\n",
    "    x_text, y_text = _compute_text_positions(ys)\n",
    "    rend.data_source.data['top'] = tops\n",
    "    rend.data_source.data['bottom'] = bottoms\n",
    "    rend.data_source.data['left'] = lefts\n",
    "    rend.data_source.data['right'] = rights\n",
    "    rend.data_source.data['keywords'] = xs\n",
    "    rend.data_source.data['relevances'] = ys\n",
    "    rend.data_source.data['x'] = x_text\n",
    "    rend.data_source.data['y'] = y_text\n",
    "    rend.data_source.data['text'] = xs\n",
    "    rend.data_source.push_notebook()\n",
    "\n",
    "def plot_relevances(data, kind, n=10, p=None, height=350):\n",
    "    \"\"\"\n",
    "    Plots keywords/concepts vs. relevance.\n",
    "    \n",
    "    Updates the plot if it has already been rendered to prevent\n",
    "    duplication.\n",
    "    \n",
    "    :param data: ([bar labels], [relevance scores])\n",
    "    :param kind: Used to label x-axis, e.g. 'Keywords' or 'Relevance'\n",
    "    :param n: Maximum number of bars to display\n",
    "    :param p: Optional already-rendered plot.\n",
    "    :param height: Optional height of the figure.\n",
    "    :return: The rendered plot.\n",
    "    \"\"\"\n",
    "    xs, ys = data\n",
    "\n",
    "    xs = xs[:n]\n",
    "    ys = ys[:n]\n",
    "    \n",
    "    if p is not None:\n",
    "        update_relevances_plot(xs, ys, p)\n",
    "        return p\n",
    "    \n",
    "    title = \"Relevant {}s\".format(kind)\n",
    "    p = figure(\n",
    "        width=1000, \n",
    "        height=height, \n",
    "        y_range=[0, 3], \n",
    "        title=title,\n",
    "        tools=\"tap\")\n",
    "    p.yaxis.axis_label = \"Relevance\"\n",
    "    p.xaxis.axis_label = kind\n",
    "    \n",
    "    tops, bottoms, lefts, rights = _compute_bars(ys)\n",
    "    x_text, y_text = _compute_text_positions(ys)\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(\n",
    "            top=tops, bottom=bottoms, left=lefts, right=rights,\n",
    "            keywords=xs, relevances=ys,\n",
    "            x=x_text,\n",
    "            y=y_text,\n",
    "            text=xs\n",
    "        ))\n",
    "    \n",
    "    p.quad(\n",
    "        top='top', bottom='bottom', left='left', right='right', \n",
    "        source=source, tags=['a'], line_color='black'\n",
    "    )\n",
    "    p.text(x='x', y='y', \n",
    "           text='text', source=source, angle=np.pi/3\n",
    "    )\n",
    "    p.xgrid[0].ticker.desired_num_ticks = len(xs)\n",
    "    p.xaxis.major_label_orientation = np.pi/4\n",
    "    \n",
    "    # Set the selected keyword using urth-core-channel when\n",
    "    # an item on the plot is clicked.\n",
    "    source.callback = CustomJS(code=\"\"\" \n",
    "        var selectedIndices = cb_obj.get('selected')['1d'].indices;\n",
    "        var allData = cb_obj.get('data');\n",
    "        var selectedKeyword = allData.keywords[selectedIndices[0]];\n",
    "\n",
    "        document.getElementById('tableChannel').set('keyword', selectedKeyword);\n",
    "        document.getElementById('plotChannel').set('selectedKeyword', selectedKeyword);\n",
    "    \"\"\")\n",
    "    \n",
    "    show(p)\n",
    "    return p\n",
    "\n",
    "def update_frequencies_plot(dates, fqs, p):\n",
    "    \"\"\"\n",
    "    Update the given frequencies plot with new data.\n",
    "    \n",
    "    dates: List of dates\n",
    "    fqs: List of frequencies\n",
    "    p: A rendered relevance plot\n",
    "    \"\"\"\n",
    "    rend = p.select(dict(type=GlyphRenderer))[0]\n",
    "    rend.data_source.data['dates'] = dates\n",
    "    rend.data_source.data['fqs'] = fqs\n",
    "    rend.data_source.push_notebook()\n",
    "\n",
    "def plot_date_frequencies(entries, p=None):\n",
    "    \"\"\"\n",
    "    Plots the number of publications per day.\n",
    "    \n",
    "    Updates the plot if it has already been rendered to prevent\n",
    "    duplication.\n",
    "    \n",
    "    :param entries: List of dicts, where each dict is an Arxiv entry\n",
    "    :param p: Optional already-rendered plot.\n",
    "    :return: The rendered plot.\n",
    "    \"\"\"\n",
    "    dates = [d.astype(datetime).date() for d in np.array(\n",
    "                [e.published for e in entries], dtype=np.datetime64)]\n",
    "\n",
    "    freqs = defaultdict(int)\n",
    "\n",
    "    for d in dates:\n",
    "        freqs[str(d)] += 1\n",
    "\n",
    "    fqs = [freqs[str(d)] for d in dates]\n",
    "\n",
    "    if p is not None:\n",
    "        update_frequencies_plot(dates, fqs, p)\n",
    "        return p\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(\n",
    "        dates=dates,\n",
    "        fqs=fqs\n",
    "    ))\n",
    "    \n",
    "    title = \"Publications per day\"\n",
    "    p = figure(\n",
    "        width=1000, \n",
    "        height=340, \n",
    "        x_axis_type=\"datetime\",\n",
    "        title=title\n",
    "    )\n",
    "    p.yaxis.axis_label = \"# Paper Submissions\"\n",
    "    p.xaxis.axis_label = \"Date\"\n",
    "    \n",
    "    # add renderers\n",
    "    p.circle('dates', 'fqs', source=source)\n",
    "    p.line('dates', 'fqs', source=source)\n",
    "\n",
    "    # show the results\n",
    "    show(p)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Watch Handler\n",
    "\n",
    "After clicking a keyword, the most relevant papers are retrieved\n",
    "and displayed in an `urth-viz-table`. This is implemented using:\n",
    "\n",
    "1. A javascript callback attached to the Bokeh plot\n",
    "2. An `urth-core-channel` `watch` handler\n",
    "\n",
    "The javascript callback sets a value that triggers the `watch` handler.\n",
    "The `watch` handler then retrieves the top documents, and `set`s a value containing\n",
    "the document data, as defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def on_plot_click(old, new):\n",
    "    global entries\n",
    "    global index\n",
    "    keyword = new\n",
    "    if keyword is not None:\n",
    "        docs, columns = top_docs_for_query(keyword, entries, index)\n",
    "        data = {\n",
    "            \"data\": docs,\n",
    "            \"columns\": columns,\n",
    "            \"timestamp\": int(round(time.time() * 1000))\n",
    "        }\n",
    "        channel('table').set('data', data)\n",
    "    \n",
    "channel('plot').watch('selectedKeyword', on_plot_click)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-level function\n",
    "\n",
    "The `run_all` function defined below executes the entire process of downloading documents, finding concepts, and plotting for a given query.\n",
    "\n",
    "We will bind to `run_all` using an `urth-core-function`, such that the function is called when a user clicks the `Run` button.\n",
    "\n",
    "The global variables are used to enable plot and data refreshing; the user can issue multiple queries without re-running the cell. Also note the calls to `channel('table').set('showTable', ...)`, which dynamically hide the table during processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concept_plot = None\n",
    "relevance_plot = None\n",
    "frequency_plot = None\n",
    "\n",
    "def run_all(query, n=NUM_PAPERS):\n",
    "    global entries\n",
    "    global index\n",
    "    global concept_plot\n",
    "    global relevance_plot\n",
    "    global frequency_plot\n",
    "    \n",
    "    channel('table').set('showTable', False)\n",
    "    channel('table').set('query', query)\n",
    "    entries = download_entries('all:{}'.format(query), n, use_cache=True)\n",
    "    index = build_tf_idf_index(entries)\n",
    "    cd = get_concepts(entries)\n",
    "    kws = get_keywords(entries)\n",
    "        \n",
    "    concept_plot = plot_relevances(\n",
    "        concepts_plot_data(cd), \n",
    "        kind=\"concept\", \n",
    "        p=concept_plot)\n",
    "    relevance_plot = plot_relevances(\n",
    "        keywords_plot_data(kws), \n",
    "        kind=\"keyword\", \n",
    "        n=15, \n",
    "        p=relevance_plot, \n",
    "        height=400)\n",
    "    frequency_plot = plot_date_frequencies(\n",
    "        entries, \n",
    "        p=frequency_plot)\n",
    "    \n",
    "    channel('status').set('status', '')\n",
    "    channel('table').set('showTable', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the concept explorer\n",
    "\n",
    "Type in a topic query, e.g. `bioinformatics`, and click `Run`. The most recent `NUM_PAPERS` papers will be downloaded from Arxiv and analyzed. \n",
    "\n",
    "The concepts and keywords will be shown in a bar chart; click on a concept / keyword or its corresponding bar to search for related papers, which will then be displayed in a table. \n",
    "\n",
    "Clicking on a table row will open the paper's Arxiv entry in a new tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<h2 style=\"text-align:center\">Arxiv Concept Explorer</h2>\n",
    "\n",
    "<div style=\"text-align:center\">Discover concepts and trends \n",
    "    in recent research papers related to a topic</div>\n",
    "\n",
    "<template is='urth-core-bind' channel='table'>\n",
    "    <urth-core-function id=\"runall\" ref=\"run_all\" arg-query=\"{{query}}\"></urth-core-function>\n",
    "    <paper-input label=\"Topic\" value=\"{{query}}\"></paper-input>\n",
    "    <paper-button raised onclick=\"runall.invoke()\">Run</button>\n",
    "</template>\n",
    "\n",
    "\n",
    "<template is='urth-core-bind' channel='status'>\n",
    "    <template is=\"dom-if\" if=\"{{status}}\">\n",
    "        <div>{{status}}</div>\n",
    "        <paper-progress value=\"{{progress}}\"></paper-progress>\n",
    "    </template>\n",
    "</template>\n",
    "\n",
    "<template is='urth-core-bind' channel='table'>\n",
    "    \n",
    "    <template is=\"dom-if\" if=\"{{showTable}}\">\n",
    "        <template is=\"dom-if\" if=\"{{keyword}}\">\n",
    "            <p>Recent <strong><span>{{query}}</span>\n",
    "                </strong> Papers Relevant to \n",
    "                <strong>\"<span>{{keyword}}</span>\"</strong> on Arxiv:</p>\n",
    "            <p>Click a row to open the paper in a new tab</p>\n",
    "            <urth-viz-table datarows=\"{{ data.data }}\" selection=\"{{sel}}\" columns=\"{{ data.columns }}\">\n",
    "                <urth-viz-col></urth-viz-col>\n",
    "                <urth-viz-col></urth-viz-col>\n",
    "                <urth-viz-col></urth-viz-col>\n",
    "                <urth-viz-col></urth-viz-col>\n",
    "                <urth-viz-col></urth-viz-col>\n",
    "            </urth-viz-table>\n",
    "            <script>\n",
    "                $('urth-viz-table').on('selection-changed', function () {\n",
    "                    var sel = $('urth-viz-table')[0].selection;\n",
    "                    var urlColumn = 3;\n",
    "                    var url = sel[urlColumn];\n",
    "                    window.open(url, '_blank');\n",
    "                });\n",
    "            </script>\n",
    "        </template>\n",
    "    </template>\n",
    "</template>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
